{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd66c44-29fc-4a56-ab03-e59f4c7ae63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"e175fedf-41be-4854-8f1a-a18d558302be\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "'use strict';\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded(error = null) {\n",
       "    const el = document.getElementById(\"e175fedf-41be-4854-8f1a-a18d558302be\");\n",
       "    if (el != null) {\n",
       "      const html = (() => {\n",
       "        if (typeof root.Bokeh === \"undefined\") {\n",
       "          if (error == null) {\n",
       "            return \"BokehJS is loading ...\";\n",
       "          } else {\n",
       "            return \"BokehJS failed to load.\";\n",
       "          }\n",
       "        } else {\n",
       "          const prefix = `BokehJS ${root.Bokeh.version}`;\n",
       "          if (error == null) {\n",
       "            return `${prefix} successfully loaded.`;\n",
       "          } else {\n",
       "            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n",
       "          }\n",
       "        }\n",
       "      })();\n",
       "      el.innerHTML = html;\n",
       "\n",
       "      if (error != null) {\n",
       "        const wrapper = document.createElement(\"div\");\n",
       "        wrapper.style.overflow = \"auto\";\n",
       "        wrapper.style.height = \"5em\";\n",
       "        wrapper.style.resize = \"vertical\";\n",
       "        const content = document.createElement(\"div\");\n",
       "        content.style.fontFamily = \"monospace\";\n",
       "        content.style.whiteSpace = \"pre-wrap\";\n",
       "        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n",
       "        content.textContent = error.stack ?? error.toString();\n",
       "        wrapper.append(content);\n",
       "        el.append(wrapper);\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(() => display_loaded(error), 100);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.0.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      try {\n",
       "            for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "\n",
       "      } catch (error) {display_loaded(error);throw error;\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"e175fedf-41be-4854-8f1a-a18d558302be\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"e175fedf-41be-4854-8f1a-a18d558302be\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"e175fedf-41be-4854-8f1a-a18d558302be\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from osgeo import gdal\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import rioxarray as rxr\n",
    "from numba import njit\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, Whisker\n",
    "from bokeh.io import output_notebook, export_png\n",
    "from bokeh.palettes import Colorblind3\n",
    "from bokeh.layouts import gridplot\n",
    "# from bokeh.layouts import gridplot, row, column\n",
    "output_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ed0062-89d5-451f-b4a5-be83bf21bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whitebox.whitebox_tools import WhiteboxTools\n",
    "\n",
    "wbt = WhiteboxTools()\n",
    "wbt.verbose = False\n",
    "# wbt.verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1723aa48-bd22-4a92-a3a6-1ab86a22e2ab",
   "metadata": {},
   "source": [
    "## Evaluate the effect of dem resolution on basin representation and attribute estimation\n",
    "\n",
    "As the basin scale decreases, methodological choices begin to have a significant impact on the number of cells captured and used to represent a basin. In addition, some attributes are affected by DEM resolution, in particular terrain attributes.  Here we investigate two examples. \n",
    "\n",
    "Consider a square grid intersected by an arbitrary curvilinear loop.  If we color the grid cells intersected by the line red, and we colour any cell inside the closed loop blue, let $P_{edge}$ the percentage of edge cells be the number of red cells divided by the number of red plus blue cells.\n",
    "\n",
    "Now if we hold the loop constant and change the grid cell dimension $d_{grid}$, intuitively $P_{edge}$ will increase as the grid cell size increases since eventually the loop will be encompassed by a single grid cell.  $P_{edge}$ will approach 1 as $d_{grid}$ increases.\n",
    "\n",
    "When we extract basin attributes from a geospatial raster using a basin polygon, at what point does a choice affecting the number of edge pixels become significant?  At some combination of raster resolution and basin size, the proportion of edge pixels will be significant.  The purpose of this exercise is to compare DEM datasets of two different resolutions to get a sense of the basin size at which the edge pixel method chosen becomes significant to the representativeness of the sample of raster pixels used to compute basin attributes.\n",
    "\n",
    "## tl;dr\n",
    "\n",
    "Compare 30m vs. 90m dem using a large sample of basin polygons over a wide scale to see when the basin is mostly edge pixels that are either included or excluded depending upon the clipping method.\n",
    "\n",
    "Once the basin clipping is done, the mean slope is computed for each basin from two different DEM sources to compare the effect of DEM resolution on mean basin slope calculation.\n",
    "\n",
    "## Method\n",
    "\n",
    "The method is as follows:\n",
    "1. Select a random sample of 10k basin polygons from the BCUB polygon set.\n",
    "2. Use each polygon as a clipping mask and create a temporary clipped DEM to represent the basin pixels. Save these as temporary files because they will be ingested by Whiteboxtools \"Slope\" tool in the subsequent slope comparison.\n",
    "3. For each clipped raster:    \n",
    "   a. Find all non-Nan indices in the clipped raster.  An edge pixel is one which has at least one NaN neighbour.  \n",
    "   b. Count all edge pixels and divide by the total number of non-NaN indices.  \n",
    "   c. The process assumes there are no data gaps in the basin prior to clipping.  This is checked by first counting NaN values for pixels within the polygon and asserting equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "787ce129-b79d-40e4-9cdf-f75cf03344e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'processed_data')\n",
    "# HYSETS_DIR = os.path.join(BASE_DIR, '/home/danbot2/code_5820/large_sample_hydrology/common_data/HYSETS_data/'\n",
    "\n",
    "# This assumes you've downloaded a basin polygon set \n",
    "# and have processed the DEM for the same region \n",
    "# with both the 3DEP and EENV DEM\n",
    "# here we'll do it for 08D (central coast)\n",
    "region_code = 'FRA'\n",
    "BASIN_DIR = os.path.join(DATA_DIR, 'derived_basins')\n",
    "buffer = 0\n",
    "# update with the path of processed DEM\n",
    "# these scripts would be already run in the dataset replication\n",
    "# see the following scripts in setup_scripts/\n",
    "#     * get_3DEP_DEM.py   <--retrieves the USGS 3DEP dem\n",
    "#     * get_EENV_DEM.py   <--retrieves the EarthEnv 90m dem\n",
    "#     * clip_region_DEM.py <-- takes the assembled tiles and creates \n",
    "\n",
    "DEM_PATH = '/home/danbot2/code_5820/large_sample_hydrology/common_data/DEM_data/processed_dem/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8719fc1c-3f1b-4504-abce-817fe2b85791",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder = os.path.join(BASE_DIR, 'validation/tmp')\n",
    "tmp_basins = os.path.join(temp_folder, 'basin_polygons/')\n",
    "\n",
    "if not os.path.exists(tmp_basins):\n",
    "    os.makedirs(tmp_basins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65a9bbaa-48e9-46ed-ba9a-eb120e6e44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_slope_on_region_raster(region_code, DEM_source):\n",
    "    \"\"\"\n",
    "    Compute the slope on the same region using two different resolution input DEMs.\n",
    "    \"\"\"\n",
    "    if region_code == 'FRA':\n",
    "        region_code = 'Fraser'\n",
    "    src_dem_fname = f'{DEM_source}/{region_code}_EENV_DEM_3005_res1.tif'\n",
    "    if 'USGS' in src:\n",
    "        src_dem_fname = f'{DEM_source}/{region_code}_{DEM_source}_3005_res1.tif'\n",
    "    # compute the mean slope of the clipped basin raster\n",
    "    output_fname = src_dem_fname.replace('res1.tif', 'slope.tif')\n",
    "    output_fpath = os.path.join(DEM_PATH, output_fname)\n",
    "    region_fpath = os.path.join(DEM_PATH, src_dem_fname)\n",
    "    if not os.path.exists(output_fpath):\n",
    "        wbt.slope(\n",
    "            region_fpath, \n",
    "            output_fpath, \n",
    "            zfactor=None, \n",
    "            units=\"degrees\", \n",
    "            callback=None\n",
    "        )\n",
    "   \n",
    "    return output_fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbd946c8-e17f-43d8-8955-c2732badbe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_slope_paths = []\n",
    "for src in ['EENV_DEM90', 'USGS_3DEP']:\n",
    "    region_slope_path = compute_slope_on_region_raster(region_code, src)\n",
    "    region_slope_paths.append(region_slope_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8262330-c76c-4060-9074-bc4edcc78172",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_fpath = f'{region_code}/{region_code}_basins_R0.parquet'\n",
    "df = gpd.read_parquet(os.path.join(BASIN_DIR, geom_fpath))\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# the ID and id columns are vestiges of the batch processing for delineating polygons\n",
    "df.drop(['ID', 'id'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9e2f897-989c-4bae-bd62-1383cba04a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the ppt_x and ppt_y columns as unique identifiers\n",
    "df['ppt_idxs'] = list(zip(df['ppt_lon_m_3005'].astype(int), df['ppt_lat_m_3005'].astype(int)))\n",
    "ppt_idxs_set = set(df['ppt_idxs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27a50fda-6dd5-4541-96b4-d36fedca3f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 basin ids (0 added)\n"
     ]
    }
   ],
   "source": [
    "# select a random subset of 10K basin polygons\n",
    "use_existing = True\n",
    "sample_size  = 1e4\n",
    "\n",
    "if not use_existing:\n",
    "    # Create a unique identifier using 'ppt_x' and 'ppt_y' as a tuple\n",
    "    sample_ids = np.random.choice(df['ppt_idxs'].values, size=int(sample_size), replace=False)\n",
    "else:\n",
    "    # Retrieve existing IDs from filenames in the temp folder\n",
    "    existing_basin_files = os.listdir(os.path.join(temp_folder, 'basin_polygons'))\n",
    "    existing_ids = set((int(e.split('_')[0]), int(e.split('_')[1])) for e in existing_basin_files)\n",
    "\n",
    "    # Calculate how many more IDs are needed\n",
    "    n_missing = max(0, sample_size - len(existing_ids))\n",
    "\n",
    "    # Get all unique IDs from the DataFrame and find the remaining ones\n",
    "    all_ids = set(map(tuple, df[['ppt_lon_m_3005', 'ppt_lat_m_3005']].values))\n",
    "    remaining_ids = np.array(list(all_ids - existing_ids))\n",
    "\n",
    "    # If more IDs are needed, select them randomly\n",
    "    additional_idxs = np.random.choice(range(len(remaining_ids)), size=int(n_missing), replace=False) if n_missing > 0 else []\n",
    "    additional_ids = remaining_ids[additional_idxs]\n",
    "\n",
    "    # Combine existing and additional IDs\n",
    "    sample_ids = np.array(list(existing_ids) + list(additional_ids))\n",
    "\n",
    "    # Print the number of IDs and how many were added\n",
    "    print(f'{len(sample_ids)} basin ids ({len(additional_ids)} added)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9bbe268-2bec-47eb-a947-f980021c2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the selected rows of the df\n",
    "# `sampled_ids` is an array of tuples: [(lat1, lon1), (lat2, lon2), ...]\n",
    "# Convert `sampled_ids` into a DataFrame for easier merging\n",
    "latlon_cols = ['ppt_lat_m_3005', 'ppt_lon_m_3005']\n",
    "for ll in latlon_cols:\n",
    "    df[ll] = df[ll].astype(int)\n",
    "# Use boolean indexing to filter rows that match the lat/lon pairs\n",
    "sample_ids = [tuple(id_pair) for id_pair in sample_ids]\n",
    "selected_df = df[df['ppt_idxs'].isin(set(sample_ids))].copy()\n",
    "# Use merge to select rows that match the lat/lon pairs in `sampled_ids`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47de8e5-2fc4-45e9-af83-c8644e14fe43",
   "metadata": {},
   "source": [
    "## Clip the DEM with each polygon and create temporary raster files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d7c0999-c6fd-4703-b429-a70cf88f4b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_raster(fpath):\n",
    "    rds = rxr.open_rasterio(fpath, masked=True, mask_and_scale=True)\n",
    "    crs = rds.rio.crs\n",
    "    affine = rds.rio.transform(recalc=False)\n",
    "    return rds, crs, affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d38e9bcd-ec1e-4daf-9d93-2314134a6978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_rasters(input_vars):\n",
    "    ppt_x, ppt_y, polygon, DEM_source, buffer, region_code, temp_folder = input_vars\n",
    "\n",
    "    dem_fname = f'{DEM_source}/{region_code}_USGS_3DEP_3005_res1.tif'\n",
    "    dem_fpath = os.path.join(DEM_PATH, dem_fname)\n",
    "    rc = region_code\n",
    "    if region_code == 'FRA':\n",
    "        rc = 'Fraser'\n",
    "    src_dem_fname = f'{DEM_source}/{rc}_EENV_DEM_3005_res1.tif'\n",
    "    if 'USGS' in src:\n",
    "        src_dem_fname = f'{DEM_source}/{rc}_{DEM_source}_3005_res1.tif'\n",
    "        \n",
    "    input_raster_path = os.path.join(DEM_PATH, src_dem_fname)\n",
    "    \n",
    "    # If buffer is needed, calculate it using raster resolution\n",
    "    if buffer != 0:\n",
    "        dem, crs_obj, affine = retrieve_raster(input_raster_path)\n",
    "        res = dem.rio.resolution()\n",
    "        buff = np.sqrt(res[0]**2 + res[1]**2) if buffer == 1 else max(abs(res[0]), abs(res[1]))\n",
    "        polygon = polygon.buffer(buff)\n",
    "\n",
    "    # save the polygon to a temp file\n",
    "    crs = 3005\n",
    "    basin_fname = f'basin_polygons/{ppt_x}_{ppt_y}_b{buffer}_basin_{crs}.shp'\n",
    "    basin_fpath = os.path.join(temp_folder, basin_fname)\n",
    "    \n",
    "    if not os.path.exists(basin_fpath):  \n",
    "        basin_data = {\n",
    "            'ppt_lon_m_3005': [ppt_x], \n",
    "            'ppt_lat_m_3005': [ppt_y], \n",
    "            'region_code': [region_code]\n",
    "        }\n",
    "        bdf = gpd.GeoDataFrame(basin_data, geometry=[polygon], crs=crs)   \n",
    "        geom_type = bdf.geometry.values[0].geom_type\n",
    "\n",
    "        if geom_type != 'Polygon':\n",
    "            print(f'geom is {geom_type}')\n",
    "            if geom_type == 'GeometryCollection':\n",
    "                foo = gpd.GeoDataFrame(geometry=[polygon], crs=3005)\n",
    "                bdf = foo.dissolve()                \n",
    "                geom_type = bdf.geometry.values[0].geom_type\n",
    "                bdf.to_file(basin_fpath)\n",
    "            elif geom_type == 'MultiPolygon':\n",
    "                print(f'multipolygon found: {basin_fpath}')\n",
    "                bdf = gpd.GeoDataFrame(geometry=[polygon], crs=3005)\n",
    "                bdf = bdf.explode(index_parts=False)\n",
    "                bdf['area'] = bdf.geometry.area\n",
    "                bdf = bdf[bdf['area'] >= 2E3]\n",
    "                if len(bdf) == 0:\n",
    "                    print(f'no polygon found {basin_fpath}')\n",
    "                    raise Exception('no geoms left!!')      \n",
    "                elif len(bdf) == 1:\n",
    "                    bdf.to_file(basin_fpath)\n",
    "                else:\n",
    "                    raise Exception('too many geoms!!')  \n",
    "                geom_type = bdf.geometry.values[0].geom_type\n",
    "            if geom_type != 'Polygon':\n",
    "                raise Exception('fix geom type!!')\n",
    "        else:\n",
    "            if not os.path.exists(basin_fpath):\n",
    "                bdf.to_file(basin_fpath)\n",
    "    # Clip the raster using the saved basin polygon\n",
    "    raster_fname = f\"{DEM_source}/{ppt_x}_{ppt_y}_clipped_{buffer}mbuff.tif\"\n",
    "    fpath_out = os.path.join(temp_folder, raster_fname)\n",
    "    if not os.path.exists(fpath_out):\n",
    "        g = gdal.Warp(fpath_out, input_raster_path, format=\"GTiff\",\n",
    "                      cutlineDSName=basin_fpath,\n",
    "                      cropToCutline=True)\n",
    "        g = None \n",
    "\n",
    "    return (fpath_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "058cf8d5-097b-4e75-9b9d-a3a62c94db17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 items selected for processing\n",
      "Processing 10000 with USGS_3DEP DEM\n",
      "Starting raster clipping:\n",
      "10000\n",
      "10000 items selected for processing\n",
      "Processing 10000 with EENV_DEM90 DEM\n",
      "Starting raster clipping:\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "tstart = time()\n",
    "\n",
    "# get all the ids\n",
    "failed_ids = []\n",
    "edge_dict = {}\n",
    "\n",
    "for src in ['USGS_3DEP', 'EENV_DEM90']:\n",
    "    tmp_raster_folder = os.path.join(temp_folder, src)\n",
    "    if not os.path.exists(tmp_raster_folder):\n",
    "        os.mkdir(tmp_raster_folder)\n",
    "    t1 = time()\n",
    "    print(f'{len(selected_df)} items selected for processing')\n",
    "    inputs = [\n",
    "        (x, y, geometry, src, buffer, region_code, temp_folder) \n",
    "        for x, y, (geometry,) in zip(selected_df['ppt_lon_m_3005'].values, selected_df['ppt_lat_m_3005'].values, selected_df[['basin_geometry']].itertuples(index=False, name=None))\n",
    "    ]\n",
    "    print(f'Processing {len(inputs)} with {src} DEM')\n",
    "\n",
    "    # clip the dems to all basin polygons\n",
    "    n_procs = 8\n",
    "    p = mp.Pool(n_procs)\n",
    "    print('Starting raster clipping:')\n",
    "    clip_fpaths = p.map(clip_rasters, inputs)\n",
    "    print(len(clip_fpaths))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae42c75-1c87-4f54-bac4-7bae46a6dd5e",
   "metadata": {},
   "source": [
    "## Count Perimeter Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d9c843e-7a9b-4c65-aa7d-df97669af5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def edge_pixel_proportion(m):\n",
    "    \"\"\"\n",
    "    The input m is a matrix representing the dem\n",
    "    where the only nonzero values are indices that\n",
    "    lie inside the basin.  \n",
    "    Count the cells that have at least one NaN neighbour\n",
    "    and compare to the total number of numeric cells.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the number of rows and columns in the matrices\n",
    "    # rows, cols = m1.shape\n",
    "    (r1, c1) = m.shape\n",
    "    # print(r1, c1)\n",
    "\n",
    "    dem_px = 0\n",
    "    edge_px = 0\n",
    "\n",
    "    # Count non-zero elements in matrix\n",
    "    for row in range(r1):\n",
    "        for col in range(c1):\n",
    "            if ~np.isnan(m[row, col]):\n",
    "                dem_px += 1\n",
    "                # check all neighbouring cells if any are nan\n",
    "                indices = [(row - 1, col - 1), (row - 1, col), (row - 1, col + 1),\n",
    "                           (row, col-1), (row, col + 1),\n",
    "                           (row + 1, col-1), (row + 1, col), (row + 1, col + 1)]\n",
    "                nan_nbr = False\n",
    "                for r, c in indices:\n",
    "                    if (r <= r1 - 1) & (r >= 0) & (c >= 0) & (c <= c1 - 1):\n",
    "                        if np.isnan(m[r, c]):\n",
    "                            edge_px += 1\n",
    "                            break\n",
    "    \n",
    "    # Return the proportion of edge pixels\n",
    "    if dem_px == 0:\n",
    "        return 0\n",
    "    return edge_px / dem_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9c3eca6-3c98-4809-ae7d-a4f366a2e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_basin_raster(f):\n",
    "    x, y = f.split('/')[-1].split('_')[:2]\n",
    "    raster, crs, affine = retrieve_raster(f)\n",
    "    data = raster.data[0]\n",
    "    pct_edge_px = edge_pixel_proportion(raster.data[0])\n",
    "    return (x, y, pct_edge_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbdd02b9-8300-4e39-b06f-d7b6d47511c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_results(src, base_dir, temp_folder):\n",
    "    \"\"\"\n",
    "    Process rasters for a given source, saving intermediate results periodically.\n",
    "    \"\"\"\n",
    "    tmp_raster_folder = os.path.join(temp_folder, src)\n",
    "    result_file = os.path.join(temp_folder, f'{src}_intermediate_results.csv')\n",
    "    processed_results = pd.DataFrame(columns=['x', 'y', f'pct_edge_cells_{src}'])\n",
    "\n",
    "    # Load existing results if available\n",
    "    if os.path.exists(result_file):\n",
    "        processed_results = pd.read_csv(result_file)\n",
    "        print(f\"Resuming {src} with {len(processed_results)} existing results.\")\n",
    "    \n",
    "    # Track already processed (x, y) pairs to avoid duplication\n",
    "    processed_ids = set(zip(processed_results['x'], processed_results['y']))\n",
    "    \n",
    "    # Get all raster file paths and sort them\n",
    "    raster_paths = sorted(\n",
    "        os.path.join(tmp_raster_folder, r) for r in os.listdir(tmp_raster_folder)\n",
    "    )\n",
    "    # Filter out already processed rasters\n",
    "    # Assuming filenames contain x and y coordinates in the format: '{x}_{y}_...'\n",
    "    remaining_paths = [\n",
    "        f for f in raster_paths \n",
    "        if (int(os.path.basename(f).split('_')[0]), int(os.path.basename(f).split('_')[1])) not in processed_ids\n",
    "    ]\n",
    "    total_to_process = len(raster_paths)\n",
    "\n",
    "    # Process remaining files and save progress\n",
    "    for i, f in enumerate(remaining_paths, 1):\n",
    "        result = process_basin_raster(f)\n",
    "        processed_results = pd.concat([processed_results, pd.DataFrame([result], columns=['x', 'y', f'pct_edge_cells_{src}'])])\n",
    "        \n",
    "        # Save every 100 results or at the end\n",
    "        if i % 500 == 0 or i == len(remaining_paths):\n",
    "            processed_results.drop_duplicates(subset=['x', 'y'], inplace=True)\n",
    "            processed_results.to_csv(result_file, index=False)\n",
    "            print(f\"{src}: {len(processed_results)} out of {total_to_process} basins processed and saved.\")\n",
    "\n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68f2c2b8-93a9-413d-86f4-86481e8852d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming EENV_DEM90 with 7965 existing results.\n",
      "EENV_DEM90: 8465 out of 17964 basins processed and saved.\n",
      "EENV_DEM90: 8965 out of 17964 basins processed and saved.\n",
      "EENV_DEM90: 9465 out of 17964 basins processed and saved.\n",
      "EENV_DEM90: 9965 out of 17964 basins processed and saved.\n",
      "EENV_DEM90: 10001 out of 17964 basins processed and saved.\n",
      "Processed EENV_DEM90 in 137s\n",
      "Final results for EENV_DEM90 saved to data/FRA_perimeter_pixel_proportion_EENV_DEM90.csv (N=10001)\n",
      "Resuming USGS_3DEP with 7964 existing results.\n",
      "USGS_3DEP: 8464 out of 10000 basins processed and saved.\n",
      "USGS_3DEP: 8964 out of 10000 basins processed and saved.\n",
      "USGS_3DEP: 9464 out of 10000 basins processed and saved.\n",
      "USGS_3DEP: 9964 out of 10000 basins processed and saved.\n",
      "USGS_3DEP: 10000 out of 10000 basins processed and saved.\n",
      "Processed USGS_3DEP in 924s\n",
      "Final results for USGS_3DEP saved to data/FRA_perimeter_pixel_proportion_USGS_3DEP.csv (N=10000)\n"
     ]
    }
   ],
   "source": [
    "# Skip processing if the final result already exists\n",
    "edge_px_results = []\n",
    "for src in ['EENV_DEM90', 'USGS_3DEP']:\n",
    "    edge_pixel_result_fpath = os.path.join('data', f'{region_code}_perimeter_pixel_proportion_{src}.csv')\n",
    "    base_dir = os.path.join(temp_folder, src)\n",
    "    if os.path.exists(edge_pixel_result_fpath):\n",
    "        result = pd.read_csv(edge_pixel_result_fpath)\n",
    "        print(f\" {len(result)} results for {src} already exist at {edge_pixel_result_fpath}\")\n",
    "    else:\n",
    "        # Process rasters for the specified source\n",
    "        t1 = time()\n",
    "        result = get_or_create_results(src, base_dir, temp_folder)\n",
    "        elapsed_time = time() - t1\n",
    "        print(f\"Processed {src} in {elapsed_time:.0f}s\")\n",
    "    \n",
    "        # Save the processed results to a CSV file for the source\n",
    "        result.to_csv(edge_pixel_result_fpath, index=False)\n",
    "        print(f\"Final results for {src} saved to {edge_pixel_result_fpath} (N={len(result)})\")\n",
    "    result.set_index(['x', 'y'], inplace=True)\n",
    "    edge_px_results.append(result)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a03a028-e446-4ff4-84b3-9868376048ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001\n"
     ]
    }
   ],
   "source": [
    "# Merge the two DataFrames on 'x' and 'y'\n",
    "edge_px_df = pd.concat(edge_px_results, axis=1)\n",
    "# Create a dictionary for fast lookups of drainage area based on (x, y) pairs from df\n",
    "area_dict = dict(zip(zip(df['ppt_lon_m_3005'], df['ppt_lat_m_3005']), df['drainage_area_km2']))\n",
    "# Map the drainage area using the (x, y) tuple as the key in edge_px_df\n",
    "edge_px_df['area'] = edge_px_df.apply(lambda row: area_dict.get(row.name), axis=1)\n",
    "print(len(edge_px_df))\n",
    "edge_px_df.head()\n",
    "edge_px_df.dropna(how='any', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adb63a9-1972-4c1d-ba26-c3d716c19a75",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3ad2d06-b9b5-4de6-ac7f-6291bcb340f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equiprobable_binning(data, param1, param2, samples_per_bin):\n",
    "    # group deviation values by perimeter bin number\n",
    "    df = data.copy()\n",
    "    n_bins = int(len(df)/samples_per_bin)\n",
    "    print(f'   Creating {n_bins} bins of {samples_per_bin} samples/bin (N={n_bins* samples_per_bin})')\n",
    "    \n",
    "    qc, edges = pd.qcut(df[param1], q=n_bins, precision=3, retbins=True)    \n",
    "    edges1 = edges[1:]\n",
    "    \n",
    "    df['p_bin'] = np.digitize(df[param1], bins=edges1, right=True)\n",
    "    \n",
    "    bin_widths = [j-i for i, j in zip(edges[:-1], edges[1:])] \n",
    "    bin_centres = [(j+i)/2 for i, j in zip(edges[:-1], edges[1:])]\n",
    "    evs = df[[param1, param2, 'p_bin']].groupby('p_bin').mean()\n",
    "\n",
    "    evs = evs.reindex(range(1, len(edges)), fill_value=np.nan)\n",
    "\n",
    "    evs['bin_width'] = bin_widths\n",
    "    evs['bin_centre'] = (edges[1:] + edges[:-1]) / 2\n",
    "    # evs['edges'] = edges\n",
    "    evs['ubnd'] = df[[param1, param2, 'p_bin']].groupby('p_bin').quantile(0.95)[param2]\n",
    "    evs['lbnd'] = df[[param1, param2, 'p_bin']].groupby('p_bin').quantile(0.05)[param2]\n",
    "    evs['q1'] = df[[param1, param2, 'p_bin']].groupby('p_bin').quantile(0.25)[param2]\n",
    "    evs['q3'] = df[[param1, param2, 'p_bin']].groupby('p_bin').quantile(0.75)[param2]\n",
    "    evs['median'] = df[[param1, param2, 'p_bin']].groupby('p_bin').quantile(0.5)[param2]\n",
    "    evs['mean'] = df[[param1, param2, 'p_bin']].groupby('p_bin').mean()[param2]\n",
    "    evs.loc[0, 'ubnd'] = evs.loc[1, 'ubnd']\n",
    "    evs.loc[0, 'lbnd'] = evs.loc[1, 'lbnd']     \n",
    "    evs.dropna(how='any', inplace=True)\n",
    "    return evs, edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f34ffac-ea6e-4643-ad7e-83d7dcf2f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binned_fig(fig, src, df, samples_per_bin=500):\n",
    "    edge_pct = f'pct_edge_cells_{src}'\n",
    "    evs, edges = equiprobable_binning(df, 'area', edge_pct, samples_per_bin)\n",
    "    \n",
    "    linetype = 'dashed'\n",
    "    color = Colorblind3[0]\n",
    "    color2 = Colorblind3[0]\n",
    "    label = '90m (EarthEnv)'\n",
    "    if src == 'USGS_3DEP':\n",
    "        label = '30m (USGS 3DEP)'\n",
    "        color = 'black'\n",
    "        linetype = 'dotted'\n",
    "        color2 = 'black'\n",
    "    \n",
    "    fig.toolbar.autohide = True\n",
    "    \n",
    "    classes = evs['bin_centre'].values\n",
    "    ub = evs['ubnd'].values\n",
    "    lb = evs['lbnd'].values\n",
    "    source = ColumnDataSource(data=dict(base=classes, upper=ub, lower=lb))\n",
    "    \n",
    "    # outlier range\n",
    "    w = Whisker(base='base', upper=\"upper\", lower=\"lower\", source=source,\n",
    "                     line_color=color, line_alpha=0.8, line_width=1)\n",
    "    w.upper_head.line_color = color\n",
    "    w.lower_head.line_color = color\n",
    "\n",
    "    if src == 'USGS_3DEP':\n",
    "        fig.circle(evs['bin_centre'], evs['median'], \n",
    "                color=color2, size=6, fill_alpha=0,\n",
    "                   legend_label=f'{label}')\n",
    "    else:\n",
    "        fig.square(evs['bin_centre'], evs['median'], \n",
    "                color=color2, size=6,\n",
    "                   legend_label=f'{label}')\n",
    "\n",
    "    fig.add_layout(w)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37e5330f-73c6-49c2-822b-dedca7169590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Creating 13 bins of 600 samples/bin (N=7800)\n",
      "   Creating 13 bins of 600 samples/bin (N=7800)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"d3b403eb-e9f7-467a-a079-addc3536ffee\" data-root-id=\"p1086\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "  const docs_json = {\"e6704cd1-495a-4b7b-870d-1917e52d2c30\":{\"version\":\"3.6.0\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1086\",\"attributes\":{\"width\":800,\"height\":650,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1088\"},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1096\",\"attributes\":{\"end\":0.5}},\"x_scale\":{\"type\":\"object\",\"name\":\"LogScale\",\"id\":\"p1097\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1098\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1089\"},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1137\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1131\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1132\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1133\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"PZqcLFQH8j9VYaOZ4S73P60Kb9ET/f4/orJj+Ig9BUAdoHLjQWQNQDZErJz1eBRA0DXzv+OLHUA+sXWfjuomQKNFwS1KpzNA38ybt/MiQ0Ds5Jx4nvdWQDdx2OUYkXhA\"},\"shape\":[12],\"dtype\":\"float64\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"PC5etgH61D9HNN7M4bfSP6yIpCFAis8/y4BCdpW/yz87Wh+n0f3HP0hz6yQqWcQ/hWAdbd6nwD8c/2OX0ea6PyQ9wOQvZbQ/pAqxJgXMrD8cgLd4dgSiP17G2rVNsIo/\"},\"shape\":[12],\"dtype\":\"float64\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1138\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1139\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1134\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"#0072B2\"},\"fill_color\":{\"type\":\"value\",\"value\":\"#0072B2\"},\"hatch_color\":{\"type\":\"value\",\"value\":\"#0072B2\"},\"marker\":{\"type\":\"value\",\"value\":\"square\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1135\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"#0072B2\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"#0072B2\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"value\",\"value\":\"#0072B2\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1},\"marker\":{\"type\":\"value\",\"value\":\"square\"}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1136\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"#0072B2\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"#0072B2\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"#0072B2\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2},\"marker\":{\"type\":\"value\",\"value\":\"square\"}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1157\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1151\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1152\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1153\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"PZqcLFQH8j9VYaOZ4S73P60Kb9ET/f4/orJj+Ig9BUAdoHLjQWQNQDZErJz1eBRA0DXzv+OLHUA+sXWfjuomQKNFwS1KpzNA38ybt/MiQ0Ds5Jx4nvdWQDdx2OUYkXhA\"},\"shape\":[12],\"dtype\":\"float64\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"uQ+JYp+Qvz8YTXn/5be7P8+GLr7TXbc/jgwSWL0WtD9HL24FzyGxP6R+8S81TK0/sT+k2OWupz/UWz0SjvSiP6ZDHyI3aJw/H/COsC0ZlD8XKh/N4MWIP2vnV4b6bXI/\"},\"shape\":[12],\"dtype\":\"float64\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1158\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1159\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1154\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"fill_color\":{\"type\":\"value\",\"value\":\"black\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1155\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"black\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1156\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"black\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1095\",\"attributes\":{\"autohide\":true,\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1109\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1110\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1111\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1112\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1118\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1117\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1119\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1120\"},{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1121\"}]}},\"toolbar_location\":\"above\",\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1104\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1105\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1106\"},\"axis_label\":\"Fraction of cells at edge\",\"axis_label_text_font\":\"Bitstream Charter\",\"axis_label_text_font_size\":\"24pt\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1107\"},\"major_label_text_font\":\"Bitstream Charter\",\"major_label_text_font_size\":\"22pt\"}}],\"below\":[{\"type\":\"object\",\"name\":\"LogAxis\",\"id\":\"p1099\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"LogTicker\",\"id\":\"p1100\",\"attributes\":{\"num_minor_ticks\":10,\"mantissas\":[1,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"LogTickFormatter\",\"id\":\"p1101\"},\"axis_label\":\"Drainage Area [km\\u00b2]\",\"axis_label_text_font\":\"Bitstream Charter\",\"axis_label_text_font_size\":\"24pt\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1102\"},\"major_label_text_font\":\"Bitstream Charter\",\"major_label_text_font_size\":\"22pt\"}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1103\",\"attributes\":{\"axis\":{\"id\":\"p1099\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1108\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1104\"}}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1140\",\"attributes\":{\"label_text_font\":\"Bitstream Charter\",\"label_text_font_size\":\"22pt\",\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1141\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"90m (EarthEnv)\"},\"renderers\":[{\"id\":\"p1137\"}]}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1160\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"30m (USGS 3DEP)\"},\"renderers\":[{\"id\":\"p1157\"}]}}]}},{\"type\":\"object\",\"name\":\"Whisker\",\"id\":\"p1125\",\"attributes\":{\"source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1122\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1123\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1124\"},\"data\":{\"type\":\"map\",\"entries\":[[\"base\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"PZqcLFQH8j9VYaOZ4S73P60Kb9ET/f4/orJj+Ig9BUAdoHLjQWQNQDZErJz1eBRA0DXzv+OLHUA+sXWfjuomQKNFwS1KpzNA38ybt/MiQ0Ds5Jx4nvdWQDdx2OUYkXhA\"},\"shape\":[12],\"dtype\":\"float64\",\"order\":\"little\"}],[\"upper\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AHPBeK5B3T+G2G07YgXZP9ffVH+fOdU/Mv5zhzRA0z9mZxDiXKjPPx5KYey7FMs/Jom48HDVxj+3eC38ff3BP0APVZM1G70/5K0zhXCPtD8zLzrWmySpP7cYypW4bJk/\"},\"shape\":[12],\"dtype\":\"float64\",\"order\":\"little\"}],[\"lower\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"TWfGJg7czT89g4klZCfLPwmn7ov0sMc/EPL7IE4gxT/+sNQIemnCP8qpxtNyer8/3F//DC+LuT+1RT3tNm61P7ZDI6OLHK8/Qse1yuxspT+lSz6unceXP5nJ9W4vyWY/\"},\"shape\":[12],\"dtype\":\"float64\",\"order\":\"little\"}]]}}},\"lower\":{\"type\":\"field\",\"field\":\"lower\"},\"lower_head\":{\"type\":\"object\",\"name\":\"TeeHead\",\"id\":\"p1129\",\"attributes\":{\"size\":{\"type\":\"value\",\"value\":10},\"line_color\":{\"type\":\"value\",\"value\":\"#0072B2\"}}},\"upper\":{\"type\":\"field\",\"field\":\"upper\"},\"upper_head\":{\"type\":\"object\",\"name\":\"TeeHead\",\"id\":\"p1130\",\"attributes\":{\"size\":{\"type\":\"value\",\"value\":10},\"line_color\":{\"type\":\"value\",\"value\":\"#0072B2\"}}},\"base\":{\"type\":\"field\",\"field\":\"base\"},\"line_color\":{\"type\":\"value\",\"value\":\"#0072B2\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.8}}},{\"type\":\"object\",\"name\":\"Whisker\",\"id\":\"p1145\",\"attributes\":{\"source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1142\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1143\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1144\"},\"data\":{\"type\":\"map\",\"entries\":[[\"base\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"PZqcLFQH8j9VYaOZ4S73P60Kb9ET/f4/orJj+Ig9BUAdoHLjQWQNQDZErJz1eBRA0DXzv+OLHUA+sXWfjuomQKNFwS1KpzNA38ybt/MiQ0Ds5Jx4nvdWQDdx2OUYkXhA\"},\"shape\":[12],\"dtype\":\"float64\",\"order\":\"little\"}],[\"upper\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"MdyYXhQCxj/f8yjSn0PCP0CgVj8l2L4/GOTwNFlyuz9kD1epw6+2PyjwMINkIrM/B8RfkbI2sD8OboPm726pP7Hqe4ceTqQ/S3mWSTXOnD8ikNuLrHiRPxMFF1nUZIE/\"},\"shape\":[12],\"dtype\":\"float64\",\"order\":\"little\"}],[\"lower\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"dQ60rnE+uD9avK7jdzO1P4G8SaachrI/Mrz4arqZrz+olwFjfoerP24DUgpOFqc/HgVAXtp1oj+9JVBkZ+KdP8DfYjG9vpU/Nf/Qs6uvjT8t2f3RbS6APzybhQ3BfEk/\"},\"shape\":[12],\"dtype\":\"float64\",\"order\":\"little\"}]]}}},\"lower\":{\"type\":\"field\",\"field\":\"lower\"},\"lower_head\":{\"type\":\"object\",\"name\":\"TeeHead\",\"id\":\"p1149\",\"attributes\":{\"size\":{\"type\":\"value\",\"value\":10}}},\"upper\":{\"type\":\"field\",\"field\":\"upper\"},\"upper_head\":{\"type\":\"object\",\"name\":\"TeeHead\",\"id\":\"p1150\",\"attributes\":{\"size\":{\"type\":\"value\",\"value\":10}}},\"base\":{\"type\":\"field\",\"field\":\"base\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.8}}}]}}]}};\n",
       "  const render_items = [{\"docid\":\"e6704cd1-495a-4b7b-870d-1917e52d2c30\",\"roots\":{\"p1086\":\"d3b403eb-e9f7-467a-a079-addc3536ffee\"},\"root_ids\":[\"p1086\"]}];\n",
       "  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    let attempts = 0;\n",
       "    const timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1086"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = figure(width=800, height=650, title=f\"\", y_range=(0, 0.5),\n",
    "           toolbar_location='above', x_axis_type='log')\n",
    "\n",
    "for src in ['EENV_DEM90', 'USGS_3DEP']:\n",
    "    fig = binned_fig(fig, src, edge_px_df, 600)\n",
    "    \n",
    "fig.xaxis.axis_label = 'Drainage Area [km]'\n",
    "fig.yaxis.axis_label = 'Fraction of cells at edge'\n",
    "\n",
    "fig.xaxis.axis_label_text_font_size = '24pt'\n",
    "fig.yaxis.axis_label_text_font_size = '24pt'\n",
    "fig.xaxis.major_label_text_font_size = '22pt'\n",
    "fig.yaxis.major_label_text_font_size = '22pt'\n",
    "fig.legend.label_text_font_size = '22pt'\n",
    "fig.yaxis.axis_label_text_font = \"Bitstream Charter\"\n",
    "fig.xaxis.axis_label_text_font = \"Bitstream Charter\"\n",
    "fig.xaxis.major_label_text_font = \"Bitstream Charter\"\n",
    "fig.yaxis.major_label_text_font = \"Bitstream Charter\"\n",
    "fig.legend.label_text_font = 'Bitstream Charter'\n",
    "\n",
    "# fig.grid.visible = False\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275e054a-85f3-469d-86c4-252efee95d4f",
   "metadata": {},
   "source": [
    "## Compute mean basin slope\n",
    "\n",
    "At the beginning of this notebook we processed the slope for the full region.  Here we compute the mean slope for each basin using basin polygons as clipping masks and compare 30 and 90m dem sources to see the effect on a large sample of basins.\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "In this example we use a random sample of 10K basins from the Frasier basin, extracted from the basin geometry file.  Note that the figure in the associated paper draws a random sample of 10K basins from the full study region.  In hindsight, it should have been seeded such that the figure could be replicated precisely.  It was not done, however at the bottom of this is a text output of the list of basin IDs so the figure below can at least be replicated. \n",
    "\n",
    "The distribution of basin slopes will vary based on the random sample drawn, but the point of the exercise is to show that the lower resolution DEM tends to compute lower slopes for the same basin, and this trend holds across all samples.  One limitation of the comparison is that we use the basin polygon derived from the higher resolution DEM, and we do not check to see if the lower resolution stream network identifies the same stream network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce3dd3b3-9bef-419c-86b0-939b89244fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['drainage_area_km2', 'ppt_lon_m_3005', 'ppt_lat_m_3005', 'ppt_acc',\n",
       "       'Perimeter_km', 'Elevation_m', 'Aspect_deg', 'Slope_deg', 'region_code',\n",
       "       'geometry', 'basin_geometry', 'centroid_geometry', 'ppt_idxs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a5b59af-ec41-4f19-a569-49595c048ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['ppt_idxs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87fe44f5-80aa-4afd-80fe-4022a9512251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_slope_raster(input_data):\n",
    "    x, y, buffer, crs, input_raster_fpath, DEM_source = input_data\n",
    "    basin_fname = f'basin_polygons/{x}_{y}_b{buffer}_basin_{crs}.shp'\n",
    "    basin_fpath = os.path.join(temp_folder, basin_fname)\n",
    "    if not os.path.exists(basin_fpath):\n",
    "        print('{basin_fpath.split(\"/\")[-1]} not found.  Saving polygon.')\n",
    "        basin = df[df['ppt_idxs'] == (x, y)].copy()\n",
    "        basin.to_file(basin_fpath)\n",
    "        print('    ...saved')\n",
    "\n",
    "    # Clip the raster using the saved basin polygon\n",
    "    raster_fname = f\"{DEM_source}/{x}_{y}_clipped_slope_{buffer}mbuff.tif\"\n",
    "    fpath_out = os.path.join(temp_folder, raster_fname)\n",
    "    if not os.path.exists(fpath_out):\n",
    "        g = gdal.Warp(fpath_out, input_raster_fpath, format=\"GTiff\",\n",
    "                      cutlineDSName=basin_fpath,\n",
    "                      cropToCutline=True)\n",
    "        g = None \n",
    "    return fpath_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be4fd3f9-408d-4e7b-9c15-904deafeff76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rasters clipped\n",
      "Processing complete for source EENV_DEM90. Results saved to data/slope_comparison_check_EENV_DEM90.csv.\n",
      "All rasters clipped\n",
      "Processed 500/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 1000/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 1500/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 2000/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 2500/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 3000/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 3500/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 4000/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 4500/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 5000/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 5500/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 6000/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 6500/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 7000/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 7500/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 8000/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 8500/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 9000/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 9500/10000 x, y pairs for source USGS_3DEP\n",
      "Processed 10000/10000 x, y pairs for source USGS_3DEP\n",
      "Processing complete for source USGS_3DEP. Results saved to data/slope_comparison_check_USGS_3DEP.csv.\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the sources\n",
    "all_slope_dfs = []\n",
    "for src in ['EENV_DEM90', 'USGS_3DEP']:\n",
    "    slope_fpath = os.path.join('data', f'slope_comparison_check_{src}.csv')\n",
    "\n",
    "    # Load existing results or create a new DataFrame\n",
    "    # Load or initialize the DataFrame\n",
    "    if os.path.exists(slope_fpath):\n",
    "        slope_df = pd.read_csv(slope_fpath)\n",
    "        \n",
    "        if 'ppt_idxs' in slope_df:\n",
    "            slope_df[['x', 'y']] = pd.DataFrame(slope_df['ppt_idxs'].tolist(), index=slope_df.index)\n",
    "        slope_df.set_index(['x', 'y'], inplace=True)\n",
    "    else:\n",
    "        slope_df = pd.DataFrame(columns=['x', 'y', src]).set_index(['x', 'y'])\n",
    "\n",
    "    # Prepare inputs and filter out processed basins\n",
    "    input_raster_fpath = [e for e in region_slope_paths if src in e][0]\n",
    "    processed_indices = set(slope_df.index)\n",
    "    slope_inputs = [\n",
    "        (i[0], i[1], buffer, 3005, input_raster_fpath, src) \n",
    "        for i in sample_ids if (i[0], i[1]) not in processed_indices\n",
    "    ]\n",
    "\n",
    "    # Skip processing if all basins are already processed\n",
    "    if not slope_inputs:\n",
    "        print(f\"All basins for source {src} already processed.\")\n",
    "        all_slope_dfs.append(slope_df)\n",
    "        continue\n",
    "\n",
    "    # Compute slope using multiprocessing\n",
    "    n_procs = 12\n",
    "    with mp.Pool(n_procs) as pool:\n",
    "        slope_paths = pool.map(clip_slope_raster, slope_inputs)\n",
    "\n",
    "    print('All rasters clipped')\n",
    "    # Process and update the DataFrame, saving every 500 iterations\n",
    "    \n",
    "    for n, path in enumerate(slope_paths, 1):\n",
    "        raster, _, _ = retrieve_raster(path)\n",
    "        mean_slope = np.nanmean(raster.data[0])\n",
    "         \n",
    "        # Extract x and y from the filename (assuming these are in the first two parts)\n",
    "        x, y = map(int, path.split('/')[-1].split('_')[:2])\n",
    "        \n",
    "        slope_df.loc[(x, y), src] = mean_slope        \n",
    "\n",
    "        # Save progress every 500 iterations\n",
    "        if n % 500 == 0:\n",
    "            slope_df.dropna(inplace=True)\n",
    "            slope_df.to_csv(slope_fpath)\n",
    "            print(f'Processed {n}/{len(slope_paths)} x, y pairs for source {src}')\n",
    "\n",
    "    # Final save\n",
    "    slope_df.dropna(inplace=True)\n",
    "    slope_df.to_csv(slope_fpath)\n",
    "    # slope_df.set_index(['x', 'y'], inplace=True)\n",
    "    all_slope_dfs.append(slope_df)\n",
    "    print(f\"Processing complete for source {src}. Results saved to {slope_fpath}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "949be0ef-e6f4-4c98-a187-d532c7ea7481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EENV_DEM90         0.0\n",
      "USGS_3DEP     0.518217\n",
      "dtype: object\n",
      "EENV_DEM90    45.0\n",
      "USGS_3DEP       45\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>EENV_DEM90</th>\n",
       "      <th>USGS_3DEP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1078226</th>\n",
       "      <th>785254</th>\n",
       "      <td>5.468461</td>\n",
       "      <td>6.970643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232235</th>\n",
       "      <th>503642</th>\n",
       "      <td>27.010790</td>\n",
       "      <td>29.342936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360939</th>\n",
       "      <th>526361</th>\n",
       "      <td>13.773086</td>\n",
       "      <td>14.443652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148707</th>\n",
       "      <th>706058</th>\n",
       "      <td>18.970585</td>\n",
       "      <td>22.02626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970901</th>\n",
       "      <th>905578</th>\n",
       "      <td>9.420944</td>\n",
       "      <td>10.851622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                EENV_DEM90  USGS_3DEP\n",
       "x       y                            \n",
       "1078226 785254    5.468461   6.970643\n",
       "1232235 503642   27.010790  29.342936\n",
       "1360939 526361   13.773086  14.443652\n",
       "1148707 706058   18.970585   22.02626\n",
       "970901  905578    9.420944  10.851622"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the two DataFrames on 'x' and 'y'\n",
    "slope_comparison_df = pd.concat(all_slope_dfs, axis=1)\n",
    "slope_comparison_df.dropna(inplace=True)\n",
    "slope_comparison_df = slope_comparison_df.clip(lower=0, upper=45)\n",
    "print(slope_comparison_df.min())\n",
    "print(slope_comparison_df.max())\n",
    "slope_comparison_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b675e42e-f708-4d19-ad1c-ac8a6523f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph1 = figure(title=f'', toolbar_location=None, \n",
    "             width=800, height=650)#, x_range=fig1.x_range)\n",
    "\n",
    "hhist1, hedges1 = np.histogram(slope_comparison_df['EENV_DEM90'].values, bins=20, density=True)\n",
    "hhist2, hedges2 = np.histogram(slope_comparison_df['USGS_3DEP'].values, bins=hedges1, density=True)\n",
    "\n",
    "ph1.xgrid.grid_line_color = None\n",
    "ph1.yaxis.major_label_orientation = np.pi/4\n",
    "ph1.background_fill_color = \"#fafafa\"\n",
    "ph1.yaxis.axis_label = 'P(X)'\n",
    "ph1.xaxis.axis_label = 'Mean Slope [deg]'\n",
    "\n",
    "\n",
    "ph1.quad(bottom=0, left=hedges1[:-1], right=hedges1[1:], top=hhist1, legend_label='90m (EarthEnv)', \n",
    "                  line_alpha=0.6, fill_alpha=0.5, color=Colorblind3[0], line_color=\"#3A5785\")\n",
    "# hh1 = ph1.quad(bottom=0, left=hedges1[:-1], right=hedges1[1:], top=hzeros1, alpha=0.5, **LINE_ARGS)\n",
    "\n",
    "ph1.quad(bottom=0, left=hedges2[:-1], right=hedges2[1:], top=hhist2, legend_label='30m (USGS 3DEP)',\n",
    "                  line_alpha=0.6, fill_alpha=0.5, color=Colorblind3[2], line_color=\"#3A5785\")\n",
    "# hh11 = ph1.quad(bottom=0, left=hedges2[:-1], right=hedges2[1:], top=hzeros2, alpha=0.5, **LINE_ARGS)\n",
    "# hh2 = ph.quad(bottom=0, left=hedges[:-1], right=hedges[1:], top=hzeros, alpha=0.1, **LINE_ARGS)\n",
    "# hh2 = ph.quad(bottom=0, left=hedges[:-1], right=hedges[1:], top=hzeros, alpha=0.1, **LINE_ARGS)\n",
    "\n",
    "ph1.xaxis.axis_label_text_font_size = '24pt'\n",
    "ph1.yaxis.axis_label_text_font_size = '24pt'\n",
    "ph1.xaxis.major_label_text_font_size = '22pt'\n",
    "ph1.yaxis.major_label_text_font_size = '22pt'\n",
    "ph1.legend.label_text_font_size = '22pt'\n",
    "ph1.yaxis.axis_label_text_font = \"Bitstream Charter\"\n",
    "ph1.xaxis.axis_label_text_font = \"Bitstream Charter\"\n",
    "ph1.xaxis.major_label_text_font = \"Bitstream Charter\"\n",
    "ph1.yaxis.major_label_text_font = \"Bitstream Charter\"\n",
    "ph1.legend.label_text_font = 'Bitstream Charter'\n",
    "\n",
    "ph1.toolbar_location = 'right'\n",
    "ph1.toolbar.autohide = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78fc6d61-ecee-44d1-8c25-e4cdeb6c2188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"cd72ba7a-f81f-4b20-ad13-4d6a96286cc0\" data-root-id=\"p1505\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "  const docs_json = {\"d3842f8c-b1b5-4c0e-a4dd-1a7ce32caf65\":{\"version\":\"3.6.0\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1505\",\"attributes\":{\"width\":800,\"height\":650,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1506\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1507\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1515\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1516\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1508\"},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1546\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1540\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1541\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1542\"},\"data\":{\"type\":\"map\",\"entries\":[[\"left\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAAAAAAAAAAAAACQAAAAAAAABJAAAAAAAAAG0AAAAAAAAAiQAAAAAAAgCZAAAAAAAAAK0AAAAAAAIAvQAAAAAAAADJAAAAAAABANEAAAAAAAIA2QAAAAAAAwDhAAAAAAAAAO0AAAAAAAEA9QAAAAAAAgD9AAAAAAADgQEAAAAAAAABCQAAAAAAAIENAAAAAAABAREAAAAAAAGBFQA==\"},\"shape\":[20],\"dtype\":\"float64\",\"order\":\"little\"}],[\"right\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAAAkAAAAAAAAASQAAAAAAAABtAAAAAAAAAIkAAAAAAAIAmQAAAAAAAACtAAAAAAACAL0AAAAAAAAAyQAAAAAAAQDRAAAAAAACANkAAAAAAAMA4QAAAAAAAADtAAAAAAABAPUAAAAAAAIA/QAAAAAAA4EBAAAAAAAAAQkAAAAAAACBDQAAAAAAAQERAAAAAAABgRUAAAAAAAIBGQA==\"},\"shape\":[20],\"dtype\":\"float64\",\"order\":\"little\"}],[\"top\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"tnlpdWj0oT/hzvlZW4K1P0RmGhFRg7I/9gWpfXgtqT8bSfRCRDOgP6LkKkSmVZg/vgg5xaYymD+KEfbM/zGXP959IFAByZY/kx1QTVUmlz+ABZxMqj2XPw03bQLERJA/y2VsT1bghj8KUDtlCw10P8ro4R4/Kl0/yujhHj8qPT9u7edL/1Q3PwAAAAAAAAAAAAAAAAAAAABu7edL/1QXPw==\"},\"shape\":[20],\"dtype\":\"float64\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1547\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1548\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1543\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"line_color\":{\"type\":\"value\",\"value\":\"#3A5785\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.6},\"fill_color\":{\"type\":\"value\",\"value\":\"#0072B2\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.5},\"hatch_color\":{\"type\":\"value\",\"value\":\"#0072B2\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1544\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"line_color\":{\"type\":\"value\",\"value\":\"#3A5785\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"#0072B2\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"value\",\"value\":\"#0072B2\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1545\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"line_color\":{\"type\":\"value\",\"value\":\"#3A5785\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"#0072B2\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"#0072B2\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1557\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1551\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1552\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1553\"},\"data\":{\"type\":\"map\",\"entries\":[[\"left\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAAAAAAAAAAAAACQAAAAAAAABJAAAAAAAAAG0AAAAAAAAAiQAAAAAAAgCZAAAAAAAAAK0AAAAAAAIAvQAAAAAAAADJAAAAAAABANEAAAAAAAIA2QAAAAAAAwDhAAAAAAAAAO0AAAAAAAEA9QAAAAAAAgD9AAAAAAADgQEAAAAAAAABCQAAAAAAAIENAAAAAAABAREAAAAAAAGBFQA==\"},\"shape\":[20],\"dtype\":\"float64\",\"order\":\"little\"}],[\"right\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAAAkAAAAAAAAASQAAAAAAAABtAAAAAAAAAIkAAAAAAAIAmQAAAAAAAACtAAAAAAACAL0AAAAAAAAAyQAAAAAAAQDRAAAAAAACANkAAAAAAAMA4QAAAAAAAADtAAAAAAABAPUAAAAAAAIA/QAAAAAAA4EBAAAAAAAAAQkAAAAAAACBDQAAAAAAAQERAAAAAAABgRUAAAAAAAIBGQA==\"},\"shape\":[20],\"dtype\":\"float64\",\"order\":\"little\"}],[\"top\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"TQ5ZVAM9hj9D/HKkrwGwP04nyEd3uLM/OS5u2RHfrT+8npFYBbGlP1juAa6bNJs/mNjQw1BhmD9aa4zesuqUP8+2RVmwmZU/qobdV1rIlT/BWRLPAOyWP4inTmBesJQ/+h65ARlckD/sRPtGUviHPwOuiFFXmnY/LC/KXAclZT9VsAtot69TP8ro4R4/Kj0/bu3nS/9UFz9u7edL/1QHPw==\"},\"shape\":[20],\"dtype\":\"float64\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1558\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1559\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1554\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"line_color\":{\"type\":\"value\",\"value\":\"#3A5785\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.6},\"fill_color\":{\"type\":\"value\",\"value\":\"#F0E442\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.5},\"hatch_color\":{\"type\":\"value\",\"value\":\"#F0E442\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1555\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"line_color\":{\"type\":\"value\",\"value\":\"#3A5785\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"#F0E442\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"value\",\"value\":\"#F0E442\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1556\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"line_color\":{\"type\":\"value\",\"value\":\"#3A5785\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"#F0E442\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"#F0E442\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1514\",\"attributes\":{\"autohide\":true,\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1527\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1528\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1529\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1530\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1536\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1535\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1537\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1538\"},{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1539\"}]}},\"toolbar_location\":\"right\",\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1522\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1523\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1524\"},\"axis_label\":\"P(X)\",\"axis_label_text_font\":\"Bitstream Charter\",\"axis_label_text_font_size\":\"24pt\",\"major_label_orientation\":0.7853981633974483,\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1525\"},\"major_label_text_font\":\"Bitstream Charter\",\"major_label_text_font_size\":\"22pt\"}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1517\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1518\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1519\"},\"axis_label\":\"Mean Slope [deg]\",\"axis_label_text_font\":\"Bitstream Charter\",\"axis_label_text_font_size\":\"24pt\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1520\"},\"major_label_text_font\":\"Bitstream Charter\",\"major_label_text_font_size\":\"22pt\"}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1521\",\"attributes\":{\"axis\":{\"id\":\"p1517\"},\"grid_line_color\":null}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1526\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1522\"}}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1549\",\"attributes\":{\"label_text_font\":\"Bitstream Charter\",\"label_text_font_size\":\"22pt\",\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1550\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"90m (EarthEnv)\"},\"renderers\":[{\"id\":\"p1546\"}]}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1560\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"30m (USGS 3DEP)\"},\"renderers\":[{\"id\":\"p1557\"}]}}]}}],\"background_fill_color\":\"#fafafa\"}}]}};\n",
       "  const render_items = [{\"docid\":\"d3842f8c-b1b5-4c0e-a4dd-1a7ce32caf65\",\"roots\":{\"p1505\":\"cd72ba7a-f81f-4b20-ad13-4d6a96286cc0\"},\"root_ids\":[\"p1505\"]}];\n",
       "  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    let attempts = 0;\n",
       "    const timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1505"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# layout = column(ph1)\n",
    "show(ph1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922306c-0c83-4fff-865e-583ebc73eafa",
   "metadata": {},
   "source": [
    "## Evaluate the Shared Region Boundary Problem\n",
    "\n",
    "The study region is broken into sub-regions, and the process of delineating all sub-basins in the BCUB begins with using the HydroBASINS polygons as a starting point.  The issue with these is they are derived from a different DEM source so the bounds are unique.  We want the regions to be delineated from a continuous source (3DEP 30m dem) and not let the HydroBASINS boundaries confine the basin delineation.\n",
    "\n",
    "To address this, we implemented an iterative process doing the following:\n",
    "\n",
    "1.  Running `merge_region_polygons.py` does a few things, but the relevant code to start is grouping the HydroBASINS polygons into sub-regions where no polygon has more than one runoff outflow, and no inflows at the edges.\n",
    "2.  Buffer the region polygons and run `clip_region_dem.py` to clip the tiled vrt dem to each buffered region.\n",
    "3.  Run the `process_flow_accumulation.py` script to derive the flow accumulation and stream networks for each region.\n",
    "4.  Run the `adjust_region_polygons` function to derive a covering set of polygons for each region.  The output is a file named `<region_code>_adjusted_covering_basins.geojson`, where the `<region_code>` is the three letter code string for the region.\n",
    "5.  Check each result from 4 against the clipped vector.  If the covering set hits the edge of the buffered clipping vector, you need to adjust the clipping bounds and repeat steps 2-4 until you have polygons that are defined by the algorithm and the DEM and not the clipping mask used at the start.  If anyone gets this far and is trying their hand at this process, I'm sorry.\n",
    "6.  Where polygons overlap or have gaps, if both buffered clipping masks fully contain the gaps/overlaps, we leave them as is for uncertainty analysis -- delineating catchment boundaries from adjacent basins turns up anomalies.  Where the edge is defined clearly on one side but intersects with the clipping mask edge, we use the side that is fully defined to trim the edge that ran up against the clipping mask.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eddd1a-2c47-4d2d-aa67-c36ec47bde72",
   "metadata": {},
   "source": [
    "### Measuring shared boundary deviations\n",
    "\n",
    "With a satisfactory set of covering polygons, we now want to make two comparisons:\n",
    "\n",
    "1.  The number and distribution of deviations between the HydroBASINS (initial) shared boundaries and the updated boundaries, and\n",
    "2.  The number and distribution of deviations in the updated set of region boundaries between shared bounds themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644ccc58-bd5f-4478-ad07-46a8abfda2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_polygon_folder = os.path.join(os.path.dirname(os.getcwd()), 'input_data')\n",
    "old_regions_fname = 'BCUB_regions_HydroBASINS.geojson'\n",
    "new_regions_fname = 'BCUB_regions_merged_R0.geojson'\n",
    "\n",
    "old_df = gpd.read_file(os.path.join(region_polygon_folder, old_regions_fname))\n",
    "new_df = gpd.read_file(os.path.join(region_polygon_folder, new_regions_fname))\n",
    "\n",
    "if not old_df.sindex:\n",
    "    old_sindex = old_df.sindex\n",
    "if not new_df.sindex:\n",
    "    new_sindex = new_df.sindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704436c5-278f-4d38-ae55-3808389cfcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb8a312-03b7-4e53-b715-8b5cae157280",
   "metadata": {},
   "source": [
    "## 1.  Old bounds vs. New\n",
    "\n",
    "I think we want to process each region individually.\n",
    "\n",
    "For each region, find all touching / intersecting neighbours. Get the intersection and the gaps (symmetric difference isn't the correct approach here).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c7e239-e1b3-4440-878a-06f1e4bb99f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import MultiPolygon, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e474d536-bedb-438a-a2cf-9bfe6a6eecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove holes from a polygon\n",
    "def remove_holes(polygon):\n",
    "    if polygon.interiors:\n",
    "        return Polygon(polygon.exterior)\n",
    "    else:\n",
    "        return polygon\n",
    "\n",
    "# Define a function to clean geometries\n",
    "def make_valid(geom):\n",
    "    if not geom.is_valid:\n",
    "        geom = geom.buffer(0)\n",
    "    return geom\n",
    "\n",
    "def fill_holes(geometry):\n",
    "    if geometry.geom_type == 'Polygon':\n",
    "        return remove_holes(geometry)\n",
    "    elif geometry.geom_type == 'MultiPolygon':\n",
    "        return MultiPolygon([remove_holes(poly) for poly in geometry])\n",
    "    else:\n",
    "        return geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea3972-ae2f-425c-94eb-b7b1ba36d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hydrobasins_intersections(target, nbr):\n",
    "    \"\"\" for each target-neighbour pairing,\n",
    "    get the intersection of the Hydrobasins region polygon\n",
    "    with the target and the covering set of the neighbour,\n",
    "    and then the intersection of the target covering set \n",
    "    with the Hydrobasins region polygon of the neighboring set. \"\"\"\n",
    "    hyd_target = old_df.loc[old_df['region_code'] == target].copy().explode(index_parts=False)\n",
    "    hyd_nbr = old_df.loc[old_df['region_code'] == nbr].copy().explode(index_parts=False)\n",
    "\n",
    "    nbr_df = new_df[new_df['region_code'] == nbr].copy().explode(index_parts=False)\n",
    "    targ_df = new_df[new_df['region_code'] == target].copy().explode(index_parts=False)\n",
    "\n",
    "    # 1. get the intersection of target (HydroBASINS) polygon with covering set of neighbour\n",
    "    print(f'    ...{target} overlay on {nbr}.')\n",
    "    int_df1 = gpd.overlay(hyd_target, nbr_df, how='intersection')\n",
    "    # 2. get the intersection of neighbour (HydroBASINS) polygon with the covering set of the target\n",
    "    print(f'    ...{nbr} overlay on {target}.')\n",
    "    int_df2 = gpd.overlay(hyd_nbr, targ_df, how='intersection')\n",
    "\n",
    "    # merge the two intersections\n",
    "    print('    ...merging the intersections.')\n",
    "    merged = gpd.GeoDataFrame(pd.concat([int_df1, int_df2]), crs=new_df.crs)\n",
    "    print('    ....intersections merged.')\n",
    "    return merged\n",
    "\n",
    "\n",
    "def merge_neighbours(df1, df2):\n",
    "    intersecting_gdf1 = gpd.sjoin(df1, df2, how='inner', op='intersects')\n",
    "    union = intersecting_gdf1.unary_union\n",
    "    union_gdf = gpd.GeoDataFrame(geometry=[union], crs=new_df.crs).explode(index_parts=False)\n",
    "    union_gdf['area'] = union_gdf.geometry.area / 1e6\n",
    "    union_gdf = union_gdf[union_gdf.index == union_gdf['area'].idxmax()]\n",
    "\n",
    "    exterior = gpd.GeoDataFrame(geometry=[Polygon(e) for e in union_gdf.geometry.exterior], crs=new_df.crs)\n",
    "    exterior['area'] = exterior.geometry.area\n",
    "    exterior = exterior[exterior.index == exterior['area'].idxmax()]\n",
    "    return exterior\n",
    "\n",
    "\n",
    "def get_self_intersections(target, nbr_df):\n",
    "    \"\"\" for each target-neighbour pairing,\n",
    "    get the set of deviations from a perfectly shared boundary.\n",
    "    This includes overlapping regions and gaps\"\"\"\n",
    "    targ_df = new_df[new_df['region_code'] == target].copy().explode(index_parts=False)\n",
    "\n",
    "    # Clean geometries\n",
    "    targ_df['geometry'] = targ_df['geometry'].apply(make_valid)\n",
    "    nbr_df['geometry'] = nbr_df['geometry'].apply(make_valid)    \n",
    "\n",
    "    # 1. get the intersections between covering sets of target and neighbour polygons\n",
    "    int_df = gpd.overlay(nbr_df, targ_df, how='intersection')\n",
    "    \n",
    "    # 2. get the holes in the shared boundary\n",
    "    # first, we merge the two regions to get the outer boundary\n",
    "    edges_1 = merge_neighbours(nbr_df, targ_df)\n",
    "    edges_2 = merge_neighbours(targ_df, nbr_df)\n",
    "    merged = gpd.GeoDataFrame(pd.concat([edges_1, edges_2]), crs=new_df.crs)\n",
    "    merged = gpd.GeoDataFrame(geometry=[merged.unary_union], crs=new_df.crs).explode()\n",
    "    \n",
    "    exterior = gpd.GeoDataFrame(geometry=[Polygon(e) for e in merged.geometry.exterior if e is not None], crs=new_df.crs)\n",
    "    exterior['area'] = exterior.geometry.area\n",
    "    if exterior.empty:\n",
    "        print('exterior is empty:')\n",
    "        print(merged)\n",
    "        return gpd.GeoDataFrame()\n",
    "    exterior = exterior[exterior.index == exterior['area'].idxmax()]\n",
    "\n",
    "    holes = gpd.overlay(exterior, merged, how='difference')\n",
    "    # merge the two components?\n",
    "    output = gpd.GeoDataFrame(pd.concat([int_df, holes]), crs=new_df.crs)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d74a7-ea32-41f0-9c08-0a7f725658a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rc in sorted(list(set(old_df['region_code']))):\n",
    "    if rc in ['HGW', 'VCI']:\n",
    "        continue\n",
    "    # get all the intersecting regions\n",
    "    \n",
    "    assert old_df.crs == new_df.crs\n",
    "    target_region = old_df.loc[old_df['region_code'] == rc].copy().explode(index_parts=False)\n",
    "    intersecting_regions = gpd.sjoin(old_df, target_region, how='inner', predicate='intersects')\n",
    "    intersecting_regions = intersecting_regions[intersecting_regions['region_code_left'] != rc].copy()\n",
    "    nbr_regions = intersecting_regions['region_code_left'].values\n",
    "\n",
    "    print(f'processing {rc}: intersects with {\", \".join(nbr_regions)}')\n",
    "\n",
    "    basin_edge_output_fname = f'data/{rc}_HydroBASINS_bounds_comparison.geojson'\n",
    "    if not os.path.exists(basin_edge_output_fname):\n",
    "        merged_intersections = []\n",
    "        for nbr in nbr_regions:\n",
    "            merged = get_hydrobasins_intersections(rc, nbr)\n",
    "            merged_intersections.append(merged)\n",
    "\n",
    "        merged_df = gpd.GeoDataFrame(pd.concat(merged_intersections), crs=new_df.crs)\n",
    "        merged_df.to_file(basin_edge_output_fname)\n",
    "\n",
    "    # get intersections with neighbouring regions between updated sets\n",
    "    boundary_dev_output_fname = f'data/{rc}_bounds_deviations_test.geojson'\n",
    "    if not os.path.exists(boundary_dev_output_fname):\n",
    "        nbr_sets = new_df[new_df['region_code'].isin(nbr_regions)].copy()\n",
    "        boundary_deviations = get_self_intersections(rc, nbr_sets)\n",
    "        boundary_deviations.to_file(boundary_dev_output_fname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b440673-5e7c-427b-afc9-eb97998790f2",
   "metadata": {},
   "source": [
    "### Analyze the uncertainty in region bounds\n",
    "\n",
    "Compare the distribution of \n",
    "\n",
    "1. deviations between the updated sub-region bounds and the initial HydroBASINS bounds, and\n",
    "2. deviations between the updated sub-region bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8643d3e-1d83-4b69-b04c-a9922c3acd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_area_distribution(df):\n",
    "    \"\"\"\n",
    "    Creates a Bokeh plot illustrating the distribution of values in the 'area' column of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing an 'area' column.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    area_values = np.sort(df['area'].dropna())\n",
    "    # Determine equiprobable bins\n",
    "    # num_bins = 20\n",
    "    cdf = np.arange(1, len(area_values) + 1) / len(area_values)\n",
    "    \n",
    "    # bin_labels, bin_edges = pd.qcut(area_values, q=num_bins, retbins=True, labels=False, duplicates='drop')\n",
    "    source = ColumnDataSource(data=dict(\n",
    "        x=area_values,\n",
    "        y=cdf\n",
    "    ))\n",
    "\n",
    "    return source\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb011d-6460-4592-91af-08d6d5b6b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = []\n",
    "plot_no = 0\n",
    "hb_dfs, bc_dfs = [], []\n",
    "for rc in sorted(list(set(old_df['region_code']))):\n",
    "    if rc in ['HGW', 'VCI']:\n",
    "        print(f' No shared boundary for {rc}')\n",
    "        continue\n",
    "    \n",
    "    print(f'Processing shared boundary analysis for {rc}')\n",
    "    hb_fname = f'data/{rc}_HydroBASINS_bounds_comparison.geojson'\n",
    "    bcub_fname = f'data/{rc}_bounds_deviations_test.geojson'\n",
    "    hb_df = gpd.read_file(hb_fname)\n",
    "    bc_df = gpd.read_file(bcub_fname)\n",
    "\n",
    "    hb_df = hb_df.explode(index_parts=False)\n",
    "    bc_df = bc_df.explode(index_parts=False)\n",
    "\n",
    "    hb_df['area'] = hb_df.geometry.area / 1e6\n",
    "    bc_df['area'] = bc_df.geometry.area / 1e6\n",
    "    hb_df = hb_df[hb_df['area'] > 0.01]\n",
    "    bc_df = bc_df[bc_df['area'] > 0.01]\n",
    "    hb_dfs.append(hb_df)\n",
    "    bc_dfs.append(bc_df)\n",
    "    \n",
    "    n_hb, n_bc = len(hb_df), len(bc_df)\n",
    "    hb_df = hb_df[hb_df.geometry.area / 1e6 > 0.01]\n",
    "    bc_df = bc_df[bc_df.geometry.area / 1e6 > 0.01]\n",
    "    n_hb1, n_bc1 = len(hb_df), len(bc_df)\n",
    "    print(f'    HydroBASINS: {n_hb1}/{n_hb} geometries after filtering area <= 0.01km2 (<1% of smallest polygon in BCUB dataset)')\n",
    "    print(f'    BCUB: {n_bc1}/{n_bc} geometries after filtering area <= 0.01km2 (<1% of smallest polygon in BCUB dataset)')\n",
    "    \n",
    "    hb_source = plot_area_distribution(hb_df)\n",
    "    bc_source = plot_area_distribution(bc_df)\n",
    "    \n",
    "    # Create a Bokeh figure\n",
    "    p = figure(title=f\"{rc} (N={len(hb_df)} HydroBASINS, {len(bc_df)} BCUB)\",\n",
    "               x_axis_label='Area [km]', x_axis_type='log',\n",
    "               y_axis_label='P(X)',\n",
    "               height=400, width=600)\n",
    "    \n",
    "    # Add a quad glyph to the figure\n",
    "    p.line(x='x', y='y', source=hb_source, line_width=2, line_color=\"#dc267f\", legend_label=\"HydroBASINS\")\n",
    "    p.line(x='x', y='y', source=bc_source, line_width=2, line_color=\"#648fff\", legend_label=\"BCUB\")\n",
    "    p.line(x=[1, 1], y=[0, 1], line_width=2, line_dash='dashed', line_color='black', legend_label='1 km')\n",
    "    p.legend.location = 'bottom_right'\n",
    "    \n",
    "    plots.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d393977-fab5-4614-a6e6-9aa4f7e5da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = gridplot(plots, ncols=3, width=400, height=225)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf834b0-2ee7-441a-8211-c0fc732fbcc6",
   "metadata": {},
   "source": [
    "Aggregate all the boundary deviations and plot the CDFs.  \n",
    "\n",
    "Ensure duplicate geometries are dropped.  Show the total numbers of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcced824-8b5f-4acf-9ae0-69d26e99cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_all_df = gpd.GeoDataFrame(pd.concat(hb_dfs), crs=new_df.crs)\n",
    "bc_all_df = gpd.GeoDataFrame(pd.concat(bc_dfs), crs=new_df.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf93edc-bb1c-4229-a772-b7825be94dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate geometries \n",
    "hb_all_df['geom_str'] = hb_all_df['geometry'].apply(lambda geom: geom.wkt)\n",
    "bc_all_df['geom_str'] = bc_all_df['geometry'].apply(lambda geom: geom.wkt)\n",
    "hb_all_df = hb_all_df.drop_duplicates(subset='geom_str')\n",
    "bc_all_df = bc_all_df.drop_duplicates(subset='geom_str')\n",
    "\n",
    "hb_all_df['area'] = hb_all_df.geometry.area / 1e6\n",
    "bc_all_df['area'] = bc_all_df.geometry.area / 1e6\n",
    "\n",
    "# drop all geometries smaller than 1% of the smallest basin in BCUB\n",
    "hb_all_df = hb_all_df[hb_all_df['area'] > 0.01]\n",
    "bc_all_df = bc_all_df[bc_all_df['area'] > 0.01]\n",
    "hb_all_df.to_file('data/HydroBASINS_all_boundary_deviations.geojson')\n",
    "bc_all_df.to_file('data/BCUB_all_boundary_deviations.geojson')\n",
    "print(len(hb_all_df), len(bc_all_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec1b201-80dd-4452-80c1-648c3a4a1879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of unique shared boundary deviations for the entire study region\n",
    "hb_source = plot_area_distribution(hb_all_df)\n",
    "bc_source = plot_area_distribution(bc_all_df)\n",
    "\n",
    "# Create a Bokeh figure\n",
    "p = figure(title='',#f\"Deviations from shared boundaries  (> 0.01 km)\",\n",
    "           x_axis_label=r'$$\\text{Area} [\\text{km}^2]$$', \n",
    "           x_axis_type='log', x_range=(10**-2.5, 10**3.35),\n",
    "           y_axis_label=r'$$P(X)$$', \n",
    "           height=900, width=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d24791-2cde-4bf1-a1cb-642efc9832a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_plot(p, hb_source, bc_source):\n",
    "    # Set the title font size\n",
    "    # p.title.text_font_size = '22pt'\n",
    "    \n",
    "    # Set the axis label font sizes\n",
    "    p.xaxis.axis_label_text_font_size = '28pt'\n",
    "    p.yaxis.axis_label_text_font_size = '28pt'\n",
    "    \n",
    "    # Set the tick label font sizes\n",
    "    p.xaxis.major_label_text_font_size = '28pt'\n",
    "    p.yaxis.major_label_text_font_size = '28pt'\n",
    "\n",
    "    \n",
    "    \n",
    "    # Add lines to the figure with increased line width for better visibility\n",
    "    p.line(x='x', y='y', source=hb_source, line_width=4, line_color=\"#dc267f\", legend_label=f\"HydroBASINS vs. BCUB (N={len(hb_all_df)})\")\n",
    "    latex_str = f\"BCUB vs. BCUB (N={len(bc_all_df)})\"\n",
    "    p.line(x='x', y='y', source=bc_source, line_width=4, line_color=\"#648fff\", legend_label=latex_str)\n",
    "    p.line(x=[1, 1], y=[0, 1], line_width=3, line_dash='dashed', line_color='black', legend_label='1 km')\n",
    "    \n",
    "    # Adjust legend font size and location\n",
    "    p.legend.label_text_font_size = '24pt'\n",
    "    p.yaxis.axis_label_text_font = \"Bitstream Charter\"\n",
    "    p.xaxis.axis_label_text_font = \"Bitstream Charter\"\n",
    "    p.xaxis.major_label_text_font = \"Bitstream Charter\"\n",
    "    p.yaxis.major_label_text_font = \"Bitstream Charter\"\n",
    "    p.legend.label_text_font = 'Bitstream Charter'\n",
    "    p.legend.location = 'bottom_right'\n",
    "    p.toolbar_location=None\n",
    "    return p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ea4da-b4ce-4e03-a8d9-be3fae7018e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = format_plot(p, hb_source, bc_source)\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb5411-73b2-43f9-83aa-32f0c5621c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_png(p, filename='edge_deviation_fig.png', width=1050, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480b956b-2f09-4baf-a52f-6b27e8a09e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_median = np.percentile(hb_all_df.geometry.area/1e6, 50)\n",
    "bc_median = np.percentile(bc_all_df.geometry.area/1e6, 50)\n",
    "print(f'Median shared boundary deviation size:\\n    HydroBASINS vs. BCUB = {hb_median:.4f} km \\n    BCUB vs. BCUB = {bc_median:.4f} km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ad4603-6342-4cb0-af7d-dcd77d306fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd4120e-13bd-4e90-bc88-05eaa3fe03c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
